{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import imageio\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mujoco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_angle_error(q1, q2):\n",
    "    # q1, q2 shape: (4,), format: [w, x, y, z]\n",
    "    dot = np.abs(np.dot(q1, q2))  # 绝对值，避免 2π 距离问题\n",
    "    dot = np.clip(dot, -1.0, 1.0)\n",
    "    angle = 2 * np.arccos(dot)\n",
    "    return abs(angle)\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.w_upward = 1\n",
    "        self.w_upfoot = 1\n",
    "        self.w_uphead = 1\n",
    "\n",
    "        self.target_orientation = np.array([1.0, 0.0, 0.0, 0.0])  # w, x, y, z\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # ------------------------\n",
    "        # 1. 躯干挺直 torso orientation (w, x, y, z) 在 obs[1:5]\n",
    "        torso_ori = obs[1:5]\n",
    "        ori_error = quaternion_angle_error(torso_ori, self.target_orientation)\n",
    "        rew_upward = np.clip(self.w_upward * ori_error, 0, 5)\n",
    "        \n",
    "        # 2. 保持脑袋高度\n",
    "        rew_uphead = np.clip(self.w_uphead * (self.env.unwrapped.data.xipos[1][2] - 0.9), -5, 5)\n",
    "        # 3. 鼓励抬脚\n",
    "        footreward = 0\n",
    "        if  0.2 <= self.env.unwrapped.data.xipos[6][2] <= 0.6:\n",
    "            footreward += 1\n",
    "        if  0.2 <= self.env.unwrapped.data.xipos[9][2] <= 0.6:\n",
    "            footreward += 1\n",
    "        rew_foot = self.w_upfoot * footreward # [0, 2]\n",
    "\n",
    "        new_reward = reward + rew_foot + rew_uphead - rew_upward\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Ant 环境l\n",
    "env = gym.make('Humanoid-v5')\n",
    "env = CustomRewardWrapper(env)\n",
    "print(f\"obs space: {env.observation_space}, action space: {env.action_space}\")\n",
    "\n",
    "\n",
    "# 评估环境\n",
    "eval_env = gym.make(\"Humanoid-v5\")   # 你的环境\n",
    "eval_env = CustomRewardWrapper(eval_env)\n",
    "\n",
    "log_dir = \"./tb_log/\"\n",
    "total_timesteps = 480000  # 总训练步数\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=log_dir+\"sac_best_model\",  # 自动保存最优模型的目录\n",
    "    log_path=log_dir,                        # 保存评估日志\n",
    "    eval_freq=10000,                          # 每 1 万步评估一次\n",
    "    n_eval_episodes=5,                         # 每次评估 5 个 episode\n",
    "    deterministic=True,                        # 评估时用确定性策略\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[64], qf=[256, 64]), # 每个隐藏层的神经元数量，也可以写成 [400, 300] 等\n",
    "    activation_fn=torch.nn.ReLU  # 激活函数，可改为 torch.nn.Tanh\n",
    ")\n",
    "\n",
    "def warm_sin_lr(progress_remaining: float) -> float:\n",
    "    \"\"\"\n",
    "    progress_remaining: 1 -> 0\n",
    "    假设总共训练T步：\n",
    "      - 前10% steps: 线性从 1e-5 升到 3e-4 (warm-up)\n",
    "      - 之后: 按正弦方式从 3e-4 降到 1e-5\n",
    "    \"\"\"\n",
    "    lr_min = 5e-5   \n",
    "    lr_max = 1e-3\n",
    "    warm_ratio = 0.01  # 10% warm-up\n",
    "\n",
    "    # progress_remaining=1 -> step=0; progress_remaining=0 -> step=end\n",
    "    progress_done = 1.0 - progress_remaining\n",
    "\n",
    "    if progress_done < warm_ratio:\n",
    "        # warm-up: 线性上升\n",
    "        return lr_min + (lr_max - lr_min) * (progress_done / warm_ratio)\n",
    "    else:\n",
    "        # sin下降：这里重新归一化到[0,1]\n",
    "        x = (progress_done - warm_ratio) / (1 - warm_ratio)\n",
    "        return lr_min + (lr_max - lr_min) * math.sin((1 - x) * math.pi / 2)\n",
    "\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate= warm_sin_lr,  # 2e-4,\n",
    "    buffer_size=1_000_000,      # 经验回放缓冲区大小. 这个参数PPO没有\n",
    "    batch_size=256,             # 默认256\n",
    "    tau=0.005,                  # 软更新系数\n",
    "    gamma=0.99,                 # 折扣因子\n",
    "    train_freq=1,               # 每步都训练，采集多少个环境步的数据后训练一次\n",
    "    gradient_steps=1,           # 对replayBuffer中读取到的batch，进行多少次梯度下降更新\n",
    "    tensorboard_log=log_dir,   # 日志目录\n",
    "    policy_kwargs=policy_kwargs,  # 将自定义结构传进去\n",
    ")\n",
    "\n",
    "# 训练模型, total_timesteps自行调整\n",
    "model.learn(total_timesteps=total_timesteps, \n",
    "            tb_log_name=\"sac_lr\", \n",
    "            progress_bar=True,\n",
    "            callback=eval_callback)\n",
    "# 保存模型\n",
    "model.save(\"sac1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapped_env = env.unwrapped\n",
    "\n",
    "mj_model = unwrapped_env.model  # MjModel\n",
    "\n",
    "print(f\"qpos size: {mj_model.nq}, qvel size: {mj_model.nv}, num_joints: {mj_model.njnt}\")  # 都是旋转关节，所以这一项都相同\n",
    "print(f\"actuator size: {mj_model.nu}, ctrl_size: {unwrapped_env.data.ctrl.shape}\")  # actuators and muscle\n",
    "print(f\"body_size: {mj_model.nbody}, body pos size: {unwrapped_env.data.xipos.shape}\")  # nbody, 3\n",
    "\n",
    "# print(f\"action range: {env.action_space.low} to {env.action_space.high}\")\n",
    "\n",
    "\n",
    "qpos_idx = 0\n",
    "for joint_id in range(mj_model.njnt):\n",
    "    joint_name = mujoco.mj_id2name(mj_model, mujoco.mjtObj.mjOBJ_JOINT, joint_id)\n",
    "    joint_type = mj_model.jnt_type[joint_id]\n",
    "    \n",
    "    # 根据关节类型确定占用的 qpos 数量\n",
    "    if joint_type == mujoco.mjtJoint.mjJNT_FREE:    # 自由关节：7个qpos (x,y,z,qw,qx,qy,qz)\n",
    "        for i, coord in enumerate(['x', 'y', 'z', 'qw', 'qx', 'qy', 'qz']):\n",
    "            print(f\"qpos[{qpos_idx:2d}]: {joint_name}_{coord}\")\n",
    "            qpos_idx += 1\n",
    "    elif joint_type == mujoco.mjtJoint.mjJNT_HINGE:  # 铰链关节：1个qpos\n",
    "        print(f\"qpos[{qpos_idx:2d}]: {joint_name}\")\n",
    "        qpos_idx += 1\n",
    "    elif joint_type == mujoco.mjtJoint.mjJNT_SLIDE:  # 滑动关节：1个qpos  \n",
    "        print(f\"qpos[{qpos_idx:2d}]: {joint_name}\")\n",
    "        qpos_idx += 1\n",
    "\n",
    "\n",
    "data  = mujoco.MjData(mj_model)\n",
    "mujoco.mj_forward(mj_model, data)  # 必须有这一步\n",
    "pos = data.xipos            # shape = (nbody, 3)\n",
    "x, y, z = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "names = [mujoco.mj_id2name(mj_model, mujoco.mjtObj.mjOBJ_BODY, i)\n",
    "         for i in range(mj_model.nbody)]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"body[{i:2d}]: {name}, pos=({x[i]:.3f}, {y[i]:.3f}, {z[i]:.3f})\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=1_000_000,      # 经验回放缓冲区大小. 这个参数PPO没有\n",
    "    batch_size=256,             # 默认256\n",
    "    tau=0.005,                  # 软更新系数\n",
    "    gamma=0.99,                 # 折扣因子\n",
    "    train_freq=1,               # 每步都训练，采集多少个环境步的数据后训练一次\n",
    "    gradient_steps=1,           # 对replayBuffer中读取到的batch，进行多少次梯度下降更新\n",
    "    tensorboard_log=\"./tb_log/\",   # 日志目录\n",
    ")\n",
    "\n",
    "# 训练模型, total_timesteps自行调整\n",
    "model.learn(total_timesteps=24000, tb_log_name=\"sac\", progress_bar=True )\n",
    "# 保存模型\n",
    "model.save(\"humanoid_sac_upward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using a GLFW raw input patch. This is not the official GLFW library.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 389/3000 [00:11<01:17, 33.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m cum_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m)):\n\u001b[0;32m---> 12\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[1;32m     14\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/envs/mujoco/mujoco_env.py:159\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Render a frame from the MuJoCo simulation as specified by the render_mode.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmujoco_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:736\u001b[0m, in \u001b[0;36mMujocoRenderer.render\u001b[0;34m(self, render_mode)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m viewer\u001b[38;5;241m.\u001b[39mrender(render_mode\u001b[38;5;241m=\u001b[39mrender_mode, camera_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_id)\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m render_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:458\u001b[0m, in \u001b[0;36mWindowViewer.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 458\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# clear overlay\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:439\u001b[0m, in \u001b[0;36mWindowViewer.render.<locals>.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m gridpos, [t1, t2] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlays\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    430\u001b[0m         mujoco\u001b[38;5;241m.\u001b[39mmjr_overlay(\n\u001b[1;32m    431\u001b[0m             mujoco\u001b[38;5;241m.\u001b[39mmjtFontScale\u001b[38;5;241m.\u001b[39mmjFONTSCALE_150,\n\u001b[1;32m    432\u001b[0m             gridpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon,\n\u001b[1;32m    437\u001b[0m         )\n\u001b[0;32m--> 439\u001b[0m \u001b[43mglfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswap_buffers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m glfw\u001b[38;5;241m.\u001b[39mpoll_events()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_per_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_per_render \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    442\u001b[0m     time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m render_start\n\u001b[1;32m    443\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/glfw/__init__.py:2380\u001b[0m, in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mswap_buffers\u001b[39m(window):\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;124;03m    Swaps the front and back buffers of the specified window.\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \n\u001b[1;32m   2377\u001b[0m \u001b[38;5;124;03m    Wrapper for:\u001b[39;00m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;124;03m        void glfwSwapBuffers(GLFWwindow* window);\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2380\u001b[0m     \u001b[43m_glfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglfwSwapBuffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torchrl/lib/python3.9/site-packages/glfw/__init__.py:687\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prepare_errcheck\u001b[39m():\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m    This function sets the errcheck attribute of all ctypes wrapped functions\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    to evaluate the _exc_info_from_callback global variable and re-raise any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    using the _callback_exception_decorator.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrcheck\u001b[39m(result, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _exc_info_from_callback\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _exc_info_from_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 使用可视化界面记录显示SAC测试结果\n",
    "# 加载模型\n",
    "model = SAC.load(\"./tb_log/sac_best_model/best_model.zip\")\n",
    "# 创建测试环境\n",
    "env = gym.make(\"Humanoid-v5\", max_episode_steps=100000, render_mode=\"human\")\n",
    "\n",
    "for i in range(5):\n",
    "    # 测试模型\n",
    "    state, info = env.reset()\n",
    "    cum_reward = 0\n",
    "    for _ in tqdm(range(3000)):\n",
    "        env.render()\n",
    "        action, _ = model.predict(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        if terminated or truncated:\n",
    "            print(\"累积奖励: \", cum_reward)\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(env._max_episode_steps)  # 输出默认最大 episode 步数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试humanoid环境\n",
    "env = gym.make(\"Humanoid-v5\", render_mode=\"human\")  # human 模式会弹出窗口\n",
    "state, info = env.reset(seed=0)\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "    reward_sum = 0\n",
    "    state, info = env.reset()\n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()  # actor选择动作\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if terminated or truncated:\n",
    "            print(\"Total reward:\", reward_sum)\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./ppo_ant_tb/\n",
    "!tensorboard --logdir ./tb_log/\n",
    "# then 然后浏览器打开 http://localhost:6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
