{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import imageio\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mujoco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_angle_error(q1, q2):\n",
    "    # q1, q2 shape: (4,), format: [w, x, y, z]\n",
    "    dot = np.abs(np.dot(q1, q2))  # 绝对值，避免 2π 距离问题\n",
    "    dot = np.clip(dot, -1.0, 1.0)\n",
    "    angle = 2 * np.arccos(dot)\n",
    "    return abs(angle)\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.w_upward = 1\n",
    "        self.w_upfoot = 1\n",
    "        self.w_uphead = 1\n",
    "\n",
    "        self.target_orientation = np.array([1.0, 0.0, 0.0, 0.0])  # w, x, y, z\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # ------------------------\n",
    "        # 1. 躯干挺直 torso orientation (w, x, y, z) 在 obs[1:5]\n",
    "        torso_ori = obs[1:5]\n",
    "        ori_error = quaternion_angle_error(torso_ori, self.target_orientation)\n",
    "        rew_upward = np.clip(self.w_upward * ori_error, 0, 5)\n",
    "        \n",
    "        # 2. 保持脑袋高度\n",
    "        rew_uphead = np.clip(self.w_uphead * (self.env.unwrapped.data.xipos[1][2] - 0.9), -5, 5)\n",
    "        # 3. 鼓励抬脚\n",
    "        footreward = 0\n",
    "        if  0.2 <= self.env.unwrapped.data.xipos[6][2] <= 0.6:\n",
    "            footreward += 1\n",
    "        if  0.2 <= self.env.unwrapped.data.xipos[9][2] <= 0.6:\n",
    "            footreward += 1\n",
    "        rew_foot = self.w_upfoot * footreward # [0, 2]\n",
    "\n",
    "        new_reward = reward + rew_foot + rew_uphead - rew_upward\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Ant 环境l\n",
    "env = gym.make('Humanoid-v5')\n",
    "env = CustomRewardWrapper(env)\n",
    "print(f\"obs space: {env.observation_space}, action space: {env.action_space}\")\n",
    "\n",
    "\n",
    "# 评估环境\n",
    "eval_env = gym.make(\"Humanoid-v5\")   # 你的环境\n",
    "eval_env = CustomRewardWrapper(eval_env)\n",
    "\n",
    "log_dir = \"./tb_log/\"\n",
    "total_timesteps = 4800000  # 总训练步数\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=log_dir+\"best_model_ppo\",  # 自动保存最优模型的目录\n",
    "    log_path=log_dir,                        # 保存评估日志\n",
    "    eval_freq=10000,                          # 每 1 万步评估一次\n",
    "    n_eval_episodes=5,                         # 每次评估 5 个 episode\n",
    "    deterministic=True,                        # 评估时用确定性策略\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[64], qf=[64]), # 每个隐藏层的神经元数量，也可以写成 [400, 300] 等\n",
    "    activation_fn=torch.nn.ReLU  # 激活函数，可改为 torch.nn.Tanh\n",
    ")\n",
    "\n",
    "def warm_sin_lr(progress_remaining: float) -> float:\n",
    "    \"\"\"\n",
    "    progress_remaining: 1 -> 0\n",
    "    假设总共训练T步：\n",
    "      - 前10% steps: 线性从 1e-5 升到 3e-4 (warm-up)\n",
    "      - 之后: 按正弦方式从 3e-4 降到 1e-5\n",
    "    \"\"\"\n",
    "    lr_min = 1e-3  \n",
    "    lr_max = 5e-3\n",
    "    warm_ratio = 0.01  # 10% warm-up\n",
    "\n",
    "    # progress_remaining=1 -> step=0; progress_remaining=0 -> step=end\n",
    "    progress_done = 1.0 - progress_remaining\n",
    "\n",
    "    if progress_done < warm_ratio:\n",
    "        # warm-up: 线性上升\n",
    "        return lr_min + (lr_max - lr_min) * (progress_done / warm_ratio)\n",
    "    else:\n",
    "        # sin下降：这里重新归一化到[0,1]\n",
    "        x = (progress_done - warm_ratio) / (1 - warm_ratio)\n",
    "        return lr_min + (lr_max - lr_min) * math.sin((1 - x) * math.pi / 2)\n",
    "\n",
    "\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=warm_sin_lr,      # 学习率，可以是固定值或调度函数\n",
    "    n_steps=4096,                    # PPO 每次 rollout 采集的步数，相当于 SAC 的 train_freq*gradient_steps\n",
    "    batch_size=256,                  # 每次梯度更新采样的 batch size\n",
    "    n_epochs=20,\n",
    "    gamma=0.99,                      # 折扣因子\n",
    "    gae_lambda=0.95,                 # GAE 参数\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.02,                     # 熵正则系数，控制探索\n",
    "    vf_coef=0.5,                      # value loss 权重\n",
    "    max_grad_norm=0.5,               # 梯度裁剪\n",
    "    tensorboard_log=log_dir,      # TensorBoard 日志目录\n",
    "    policy_kwargs=policy_kwargs       # 自定义网络结构\n",
    ")\n",
    "\n",
    "# 训练模型, total_timesteps自行调整\n",
    "model.learn(total_timesteps=total_timesteps, \n",
    "            tb_log_name=\"ppo\", \n",
    "            progress_bar=True,\n",
    "            callback=eval_callback)\n",
    "# 保存模型\n",
    "model.save(\"humanoid_ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gym.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using a GLFW raw input patch. This is not the official GLFW library.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 31/1500 [00:00<00:46, 31.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积奖励:  145.94207894300038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 34/1500 [00:00<00:22, 63.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积奖励:  150.91537351579376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 66/1500 [00:01<00:34, 41.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积奖励:  280.50079320431576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1500 [00:00<00:51, 28.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积奖励:  99.73514584014339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 51/1500 [00:01<00:49, 29.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积奖励:  239.38347088550447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用可视化界面记录显示SAC测试结果\n",
    "# 加载模型\n",
    "model = PPO.load(\"./humanoid_ppo.zip\")\n",
    "# 创建测试环境\n",
    "env = gym.make(\"Humanoid-v5\", render_mode=\"human\")\n",
    "\n",
    "for i in range(5):\n",
    "    # 测试模型\n",
    "    state, info = env.reset()\n",
    "    cum_reward = 0\n",
    "    for _ in tqdm(range(1500)):\n",
    "        env.render()\n",
    "        action, _ = model.predict(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        if terminated or truncated:\n",
    "            print(\"累积奖励: \", cum_reward)\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./ppo_ant_tb/\n",
    "!tensorboard --logdir ./tb_log/\n",
    "# then 然后浏览器打开 http://localhost:6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
