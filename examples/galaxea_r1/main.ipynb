{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "import mujoco\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id=\"galaxea_r1Pro\",\n",
    "    entry_point=\"galaxea_r1Pro:Galaxea_r1Pro\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仿真交互环境\n",
    "train_env = gym.make(\"galaxea_r1Pro\")\n",
    "obs, _ = train_env.reset()\n",
    "print(f\"obs space: {train_env.observation_space.shape}, action space: {train_env.action_space.shape}\")\n",
    "\n",
    "\n",
    "log_dir = \"./tb_log/\"\n",
    "\n",
    "total_timesteps = 2400000  # 总训练步数\n",
    "\n",
    "# 评估环境\n",
    "eval_env = gym.make(\"galaxea_r1Pro\")   # 你的环境\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=log_dir+\"best_model2\",  # 自动保存最优模型的目录\n",
    "    log_path=log_dir,                        # 保存评估日志\n",
    "    eval_freq=total_timesteps // 10,                          # 每 1 万步评估一次\n",
    "    n_eval_episodes=5,                         # 每次评估 5 个 episode\n",
    "    deterministic=True,                        # 评估时用确定性策略\n",
    "    render=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardInfoCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    在 TensorBoard 中记录 info 字典中的各个 reward 项。\n",
    "    支持 vectorized environments。\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        if not infos:\n",
    "            return True\n",
    "\n",
    "        # 对 vectorized env 平均 reward\n",
    "        reward_sums = {}\n",
    "        for info in infos:\n",
    "            for key, value in info.items():\n",
    "                if key.startswith(\"reward_\"):\n",
    "                    reward_sums[key] = reward_sums.get(key, 0.0) + float(value)\n",
    "\n",
    "        for key, total in reward_sums.items():\n",
    "            mean_value = total / len(infos)\n",
    "            self.logger.record(f\"reward/{key}\", mean_value)\n",
    "\n",
    "        return True\n",
    "    \n",
    "reward_info_cb = RewardInfoCallback()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自定义SAC网络结构\n",
    "# obs space: (706,), action space: (24,)\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[256, 256], qf=[256, 256]), # 每个隐藏层的神经元数量，也可以写成 [400, 300] 等\n",
    "    activation_fn=torch.nn.ReLU  # 激活函数，可改为 torch.nn.Tanh\n",
    ")\n",
    "\n",
    "def warm_sin_lr(progress_remaining: float) -> float:\n",
    "    \"\"\"\n",
    "    progress_remaining: 1 -> 0\n",
    "    假设总共训练T步：\n",
    "      - 前10% steps: 线性从 1e-5 升到 3e-4 (warm-up)\n",
    "      - 之后: 按正弦方式从 3e-4 降到 1e-5\n",
    "    \"\"\"\n",
    "    lr_min = 1e-4   \n",
    "    lr_max = 1e-3\n",
    "    warm_ratio = 0.01  # 10% warm-up\n",
    "\n",
    "    # progress_remaining=1 -> step=0; progress_remaining=0 -> step=end\n",
    "    progress_done = 1.0 - progress_remaining\n",
    "\n",
    "    if progress_done < warm_ratio:\n",
    "        # warm-up: 线性上升\n",
    "        return lr_min + (lr_max - lr_min) * (progress_done / warm_ratio)\n",
    "    else:\n",
    "        # sin下降：这里重新归一化到[0,1]\n",
    "        x = (progress_done - warm_ratio) / (1 - warm_ratio)\n",
    "        return lr_min + (lr_max - lr_min) * math.sin((1 - x) * math.pi / 2)\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    verbose=1,\n",
    "    learning_rate=warm_sin_lr,\n",
    "    buffer_size=2_000_000,      # 经验回放缓冲区大小. 这个参数PPO没有\n",
    "    learning_starts=1000, \n",
    "    batch_size=256,             # 默认256\n",
    "    tau=0.005,                  # 软更新系数\n",
    "    gamma=0.99,                 # 折扣因子\n",
    "    train_freq=1,               # 每步都训练，采集多少个环境步的数据后训练一次\n",
    "    gradient_steps=1,           # 对replayBuffer中读取到的batch，进行多少次梯度下降更新\n",
    "    tensorboard_log=log_dir,   # 日志目录\n",
    "    policy_kwargs=policy_kwargs,  # 将自定义结构传进去\n",
    ")\n",
    "\n",
    "\n",
    "# 训练模型, total_timesteps自行调整\n",
    "model.learn(total_timesteps=total_timesteps, \n",
    "            tb_log_name=\"sac\", \n",
    "            progress_bar=True,\n",
    "            callback=[eval_callback, reward_info_cb])\n",
    "# 保存模型\n",
    "model.save(\"galaxea_sac_lr_forward\")\n",
    "model.save_replay_buffer(\"my_buffer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "model = SAC.load(\"galaxea_sac_lr_forward\", env=train_env, tensorboard_log=log_dir)\n",
    "model.load_replay_buffer(\"my_buffer.pkl\")\n",
    "model.learning_starts = 0  # 继续训练时，不需要再等待采集数据了,因为已经用了之前的replaybuffer\n",
    "model.learning_rate = 1e-4\n",
    "model.learn(total_timesteps=240000,\n",
    "            tb_log_name=\"sac\",\n",
    "            progress_bar=True,\n",
    "            callback=[eval_callback, reward_info_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用可视化界面记录显示SAC测试结果\n",
    "# 加载模型\n",
    "model = SAC.load(\"./tb_log/best_model/best_model.zip\")\n",
    "# 创建测试环境\n",
    "visual_env = gym.make(\"galaxea_r1Pro\", render_mode=\"human\")\n",
    "\n",
    "for i in range(5):\n",
    "    # 测试模型\n",
    "    obs, info = visual_env.reset()\n",
    "    cum_reward = 0\n",
    "    for _ in tqdm(range(1500)):\n",
    "        visual_env.render()\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        next_obs, reward, terminated, truncated, info = visual_env.step(action)\n",
    "        cum_reward += reward\n",
    "        if terminated or truncated:\n",
    "            print(\"累积奖励: \", cum_reward)\n",
    "            break\n",
    "            \n",
    "        obs = next_obs\n",
    "\n",
    "visual_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化环境\n",
    "train_env = gym.make(\"galaxea_r1Pro\", render_mode=\"human\")\n",
    "unwrapped_env = train_env.unwrapped\n",
    "\n",
    "model = unwrapped_env.model  # MjModel\n",
    "\n",
    "print(f\"obs space: {train_env.observation_space.shape}, action space: {train_env.action_space.shape}\")\n",
    "print(f\"action range: {train_env.action_space.low} to {train_env.action_space.high}\")\n",
    "\n",
    "\n",
    "print(f\"actuator size: {model.nu}, ctrl_size: {unwrapped_env.data.ctrl.shape}\")  # actuators and muscles\n",
    "print(f\"obs space: {train_env.observation_space.shape}, action space: {train_env.action_space.shape}\")\n",
    "\n",
    "\n",
    "# 随机采样动作\n",
    "obs, _ = train_env.reset()\n",
    "for _ in tqdm(range(1000)):\n",
    "    train_env.render()\n",
    "    action = train_env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = train_env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = train_env.reset()\n",
    "train_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qpos size: 27, qvel size: 27, num_joints: 27\n",
      "actuator size: 21, ctrl_size: (21,)\n",
      "body_size: 26, body pos size: (26, 3)\n",
      "qpos[ 0]: base_x\n",
      "qpos[ 1]: base_y\n",
      "qpos[ 2]: base_yaw\n",
      "qpos[ 3]: steer_motor_joint1\n",
      "qpos[ 4]: wheel_motor_joint1\n",
      "qpos[ 5]: steer_motor_joint2\n",
      "qpos[ 6]: wheel_motor_joint2\n",
      "qpos[ 7]: steer_motor_joint3\n",
      "qpos[ 8]: wheel_motor_joint3\n",
      "qpos[ 9]: torso_joint1\n",
      "qpos[10]: torso_joint2\n",
      "qpos[11]: torso_joint3\n",
      "qpos[12]: torso_joint4\n",
      "qpos[13]: left_arm_joint1\n",
      "qpos[14]: left_arm_joint2\n",
      "qpos[15]: left_arm_joint3\n",
      "qpos[16]: left_arm_joint4\n",
      "qpos[17]: left_arm_joint5\n",
      "qpos[18]: left_arm_joint6\n",
      "qpos[19]: left_arm_joint7\n",
      "qpos[20]: right_arm_joint1\n",
      "qpos[21]: right_arm_joint2\n",
      "qpos[22]: right_arm_joint3\n",
      "qpos[23]: right_arm_joint4\n",
      "qpos[24]: right_arm_joint5\n",
      "qpos[25]: right_arm_joint6\n",
      "qpos[26]: right_arm_joint7\n",
      "body[ 0]: world, pos=(0.000, 0.000, 0.000)\n",
      "body[ 1]: base_link, pos=(-0.056, -0.000, 0.206)\n",
      "body[ 2]: steer_motor_link1, pos=(0.169, 0.275, 0.108)\n",
      "body[ 3]: wheel_motor_link1, pos=(0.169, 0.280, 0.070)\n",
      "body[ 4]: steer_motor_link2, pos=(0.169, -0.280, 0.105)\n",
      "body[ 5]: wheel_motor_link2, pos=(0.169, -0.280, 0.070)\n",
      "body[ 6]: steer_motor_link3, pos=(-0.327, -0.000, 0.105)\n",
      "body[ 7]: wheel_motor_link3, pos=(-0.327, 0.000, 0.070)\n",
      "body[ 8]: torso_link1, pos=(-0.078, 0.003, 0.574)\n",
      "body[ 9]: torso_link2, pos=(-0.074, 0.014, 0.900)\n",
      "body[10]: torso_link3, pos=(-0.079, -0.005, 1.063)\n",
      "body[11]: torso_link4, pos=(-0.078, -0.000, 1.437)\n",
      "body[12]: left_arm_link1, pos=(-0.079, 0.245, 1.445)\n",
      "body[13]: left_arm_link2, pos=(-0.081, 0.248, 1.413)\n",
      "body[14]: left_arm_link3, pos=(-0.074, 0.251, 1.217)\n",
      "body[15]: left_arm_link4, pos=(-0.081, 0.251, 1.093)\n",
      "body[16]: left_arm_link5, pos=(-0.080, 0.249, 0.977)\n",
      "body[17]: left_arm_link6, pos=(-0.078, 0.251, 0.877)\n",
      "body[18]: left_arm_link7, pos=(-0.062, 0.246, 0.768)\n",
      "body[19]: right_arm_link1, pos=(-0.080, -0.245, 1.445)\n",
      "body[20]: right_arm_link2, pos=(-0.078, -0.247, 1.413)\n",
      "body[21]: right_arm_link3, pos=(-0.074, -0.253, 1.217)\n",
      "body[22]: right_arm_link4, pos=(-0.080, -0.253, 1.078)\n",
      "body[23]: right_arm_link5, pos=(-0.080, -0.249, 0.967)\n",
      "body[24]: right_arm_link6, pos=(-0.078, -0.251, 0.877)\n",
      "body[25]: right_arm_link7, pos=(-0.062, -0.258, 0.768)\n"
     ]
    }
   ],
   "source": [
    "train_env = gym.make(\"galaxea_r1Pro\")\n",
    "obs, _ = train_env.reset()\n",
    "unwrapped_env = train_env.unwrapped\n",
    "\n",
    "mj_model = unwrapped_env.model  # MjModel\n",
    "\n",
    "print(f\"qpos size: {mj_model.nq}, qvel size: {mj_model.nv}, num_joints: {mj_model.njnt}\")  # 都是旋转关节，所以这一项都相同\n",
    "print(f\"actuator size: {mj_model.nu}, ctrl_size: {unwrapped_env.data.ctrl.shape}\")  # actuators and muscle\n",
    "print(f\"body_size: {mj_model.nbody}, body pos size: {unwrapped_env.data.xipos.shape}\")  # nbody, 3\n",
    "\n",
    "# print(f\"action range: {env.action_space.low} to {env.action_space.high}\")\n",
    "\n",
    "\n",
    "qpos_idx = 0\n",
    "for joint_id in range(mj_model.njnt):\n",
    "    joint_name = mujoco.mj_id2name(mj_model, mujoco.mjtObj.mjOBJ_JOINT, joint_id)\n",
    "    joint_type = mj_model.jnt_type[joint_id]\n",
    "    \n",
    "    # 根据关节类型确定占用的 qpos 数量\n",
    "    if joint_type == mujoco.mjtJoint.mjJNT_FREE:    # 自由关节：7个qpos (x,y,z,qw,qx,qy,qz)\n",
    "        for i, coord in enumerate(['x', 'y', 'z', 'qw', 'qx', 'qy', 'qz']):\n",
    "            print(f\"qpos[{qpos_idx:2d}]: {joint_name}_{coord}\")\n",
    "            qpos_idx += 1\n",
    "    elif joint_type == mujoco.mjtJoint.mjJNT_HINGE:  # 铰链关节：1个qpos\n",
    "        print(f\"qpos[{qpos_idx:2d}]: {joint_name}\")\n",
    "        qpos_idx += 1\n",
    "    elif joint_type == mujoco.mjtJoint.mjJNT_SLIDE:  # 滑动关节：1个qpos  \n",
    "        print(f\"qpos[{qpos_idx:2d}]: {joint_name}\")\n",
    "        qpos_idx += 1\n",
    "\n",
    "\n",
    "data  = mujoco.MjData(mj_model)\n",
    "mujoco.mj_forward(mj_model, data)  # 必须有这一步\n",
    "pos = data.xipos            # shape = (nbody, 3)\n",
    "x, y, z = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "names = [mujoco.mj_id2name(mj_model, mujoco.mjtObj.mjOBJ_BODY, i)\n",
    "         for i in range(mj_model.nbody)]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"body[{i:2d}]: {name}, pos=({x[i]:.3f}, {y[i]:.3f}, {z[i]:.3f})\")\n",
    "\n",
    "\n",
    "train_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
